{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation on Amazon SageMaker with Factorization Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM1: Download ml-100k data  \n",
    "***The data sets are needed to train our Factorization Machine. We use the 100,000 movie ratings given by users from MovieLens data sets. This dataset has been pre curated on the Spark-preprocessing notebook and uploaded to an S3 bucket***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Information\n",
    "*ua.base (train.csv): data for training*  \n",
    "*ua.test (test.csv): data for test/validation*  \n",
    "*Headers/columns :* ***user id | item id | rating (1-5) | timestamp | rating_b (0,1)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-30 16:56:09          0 _SUCCESS\r\n",
      "2019-05-30 16:56:09    1973623 part-00000-b35d2537-f655-4056-a3f7-e2c49a0b0a9a-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://pedro-spark-sagemaker/raw/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-30 17:14:30          0 _SUCCESS\r\n",
      "2019-05-30 17:14:30     205513 part-00000-f5786d74-515f-4b6d-b42a-dd5612d611b1-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://pedro-spark-sagemaker/raw/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/1.9 MiB (3.6 MiB/s) with 1 file(s) remaining\r",
      "Completed 512.0 KiB/1.9 MiB (6.9 MiB/s) with 1 file(s) remaining\r",
      "Completed 768.0 KiB/1.9 MiB (9.9 MiB/s) with 1 file(s) remaining\r",
      "Completed 1.0 MiB/1.9 MiB (12.7 MiB/s) with 1 file(s) remaining \r",
      "Completed 1.2 MiB/1.9 MiB (15.4 MiB/s) with 1 file(s) remaining \r",
      "Completed 1.5 MiB/1.9 MiB (18.0 MiB/s) with 1 file(s) remaining \r",
      "Completed 1.8 MiB/1.9 MiB (20.6 MiB/s) with 1 file(s) remaining \r",
      "Completed 1.9 MiB/1.9 MiB (22.0 MiB/s) with 1 file(s) remaining \r",
      "download: s3://pedro-spark-sagemaker/raw/train/part-00000-b35d2537-f655-4056-a3f7-e2c49a0b0a9a-c000.csv to ./train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://pedro-spark-sagemaker/raw/train/part-00000-b35d2537-f655-4056-a3f7-e2c49a0b0a9a-c000.csv train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 200.7 KiB/200.7 KiB (4.2 MiB/s) with 1 file(s) remaining\r",
      "download: s3://pedro-spark-sagemaker/raw/test/part-00000-f5786d74-515f-4b6d-b42a-dd5612d611b1-c000.csv to ./test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://pedro-spark-sagemaker/raw/test/part-00000-f5786d74-515f-4b6d-b42a-dd5612d611b1-c000.csv test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "df= pd.read_csv('train.csv',names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP','RATING_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= pd.read_csv('test.csv',names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP','RATING_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>RATING_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>878542699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>878542420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>943</td>\n",
       "      <td>808</td>\n",
       "      <td>4</td>\n",
       "      <td>888639868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>943</td>\n",
       "      <td>1067</td>\n",
       "      <td>2</td>\n",
       "      <td>875501756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9429 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      USER_ID  ITEM_ID  RATING  TIMESTAMP  RATING_B\n",
       "0           1       33       4  878542699         1\n",
       "1           1       61       4  878542420         1\n",
       "...       ...      ...     ...        ...       ...\n",
       "9427      943      808       4  888639868         1\n",
       "9428      943     1067       2  875501756         0\n",
       "\n",
       "[9429 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER_ID</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>RATING_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90567</th>\n",
       "      <td>943</td>\n",
       "      <td>1228</td>\n",
       "      <td>3</td>\n",
       "      <td>888640275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90568</th>\n",
       "      <td>943</td>\n",
       "      <td>1330</td>\n",
       "      <td>3</td>\n",
       "      <td>888692465</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90569 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       USER_ID  ITEM_ID  RATING  TIMESTAMP  RATING_B\n",
       "0            1        2       3  876893171         0\n",
       "1            1        3       4  878542960         1\n",
       "...        ...      ...     ...        ...       ...\n",
       "90567      943     1228       3  888640275         0\n",
       "90568      943     1330       3  888692465         0\n",
       "\n",
       "[90569 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 5)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM3: Build training set and test set\n",
    "***Import necessary modules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, csv, io, json\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set S3 bucket and prefix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::349934754982:role/service-role/AmazonSageMaker-ExecutionRole-20190509T114602\n",
      "CPU times: user 149 ms, sys: 8.32 ms, total: 157 ms\n",
      "Wall time: 3.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'factorization-machine-sagemaker'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize number of total users and movies in data set, as well as number of train and test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbUsers=943\n",
    "nbMovies=1682\n",
    "nbFeatures=nbUsers+nbMovies\n",
    "\n",
    "nbRatingsTrain=90570\n",
    "nbRatingsTest=9430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM4: Define method to load dataset\n",
    "\n",
    "***The data will be loaded into 2 vectors: feature vector X and label vector Y***  \n",
    "Feature vector X is a one-hot encoded vector that sticks and flattens user Ids and movie Ids together. It should look like this below (without rows' and columns' labels):   \n",
    "\n",
    "|<pre></pre>| 1 \t    |      2    |    3  \t|<pre>...</pre>| 1  | 2  |<pre>...</pre>|\n",
    "|   :---:   |:---:      |    :---:\t|   :---:\t|  :---:\t|  :---:\t |  :---:     |   :---:\t  |\n",
    "| **data0** | 1 \t    |<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre> |<pre>1</pre>|<pre></pre>|\n",
    "| **data1** |<pre></pre>| 1 \t    |<pre></pre>|<pre></pre>|<pre>1</pre>|<pre></pre> |<pre></pre>|\n",
    "|<pre>...</pre>|<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre>|<pre></pre> |<pre></pre> |<pre></pre>|\n",
    " \n",
    "It is a 2D sparse matrix where columns are user Ids and movie Ids, and rows are data items in the training/test data set.\n",
    "One row represents 1 training/test data that has 2 ones (1s) that mark the user Id and movie Id that he/she rated.   \n",
    "\n",
    "Label vector Y is a 1D vector containing expected output. It looks like this below (without rows' labels):\n",
    "\n",
    "|<pre></pre>|<pre></pre>|\n",
    "| :--- | :---:|\n",
    "|**data0**| 1 |\n",
    "|**data1**| 1 |\n",
    "|**data2**| 0 |\n",
    "|**data3**| 1 |\n",
    "|<pre>...</pre>|<pre></pre>|\n",
    "\n",
    "\n",
    "If user's rating for that movie is 4 or 5, then value is 1, otherwise 0. Each element corresponds to one data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadDataset(filename, lines, columns):\n",
    "\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines-1, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter=',')\n",
    "        for userId,movieId,rating,timestamp,rating_b in samples:\n",
    "            if line==0:\n",
    "                Y.append(rating_b)\n",
    "                line=line+1\n",
    "            else:\n",
    "                Y.append(rating_b)\n",
    "                X[line-1,int(userId)-1] = 1\n",
    "                X[line-1,int(nbUsers)+int(movieId)-1] = 1\n",
    "                line=line+1\n",
    "            \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    \n",
    "    return X,Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now that we have defined the loadDataset method, lets load both training and test data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset('train.csv', nbRatingsTrain, nbFeatures)\n",
    "X_test, Y_test = loadDataset('test.csv',nbRatingsTest,nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's examine the dimensions of X and Y vectors***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90569, 2625)\n",
      "(90569,)\n",
      "Training labels: 49905 zeros, 40665 ones\n",
      "(9429, 2625)\n",
      "(9429,)\n",
      "Test labels: 5468 zeros, 3962 ones\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (nbRatingsTrain-1, nbFeatures)\n",
    "assert Y_train.shape == (nbRatingsTrain-1, )\n",
    "zero_labels = np.count_nonzero(Y_train)\n",
    "print(\"Training labels: %d zeros, %d ones\" % (zero_labels, nbRatingsTrain-zero_labels))\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (nbRatingsTest-1, nbFeatures)\n",
    "assert Y_test.shape  == (nbRatingsTest-1, )\n",
    "zero_labels = np.count_nonzero(Y_test)\n",
    "print(\"Test labels: %d zeros, %d ones\" % (zero_labels, nbRatingsTest-zero_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM5: Convert to protobuf and save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train3')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test3')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-southeast-1-349934754982/factorization-machine-sagemaker/train3/train.protobuf\n",
      "s3://sagemaker-ap-southeast-1-349934754982/factorization-machine-sagemaker/test3/test.protobuf\n",
      "Output: s3://sagemaker-ap-southeast-1-349934754982/factorization-machine-sagemaker/output\n"
     ]
    }
   ],
   "source": [
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM6: Run training job\n",
    "***We are done with the data preparation part. Let's begin training our Factorization Machine model.***  \n",
    "***SageMaker provides both the container and built-in algorithm to run the training and inference.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest',\n",
    "              'ap-southeast-1': '475088953585.dkr.ecr.ap-southeast-1.amazonaws.com/factorization-machines:latest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Behing the scene, SageMaker provisions a container to run the training, and terminate it after training job succeeds. Metrics during training, including accuracy are posted to CloudWatch Metrics.***    \n",
    "\n",
    "Note: If you like GUI (Graphical User Interface), you can execute the training via AWS Console too. Basically we can interact with AWS in 3 ways: AWS Console (GUI), CLI, and SDK. For this lab, we are using SDK. You can inspect https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs (change the region as necessary) to see the running training job after you run the step below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-30 17:22:41 Starting - Starting the training job...\n",
      "2019-05-30 17:22:42 Starting - Launching requested ML instances.........\n",
      "2019-05-30 17:24:17 Starting - Preparing the instances for training......\n",
      "2019-05-30 17:25:19 Downloading - Downloading input data...\n",
      "2019-05-30 17:26:12 Training - Training image download completed. Training in progress.\n",
      "2019-05-30 17:26:12 Uploading - Uploading generated training model\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'10', u'feature_dim': u'2625', u'mini_batch_size': u'1000', u'predictor_type': u'binary_classifier', u'num_factors': u'64'}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'10', u'feature_dim': u'2625', u'num_factors': u'64', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'binary_classifier', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 WARNING 140290400139072] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:03.655] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:03.663] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] nvidia-smi took: 0.0503981113434 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 61.03205680847168, \"sum\": 61.03205680847168, \"min\": 61.03205680847168}}, \"EndTime\": 1559237163.721642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237163.650965}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1559237163.721827, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237163.721773}\n",
      "\u001b[0m\n",
      "\u001b[31m[17:26:03] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.201496.0/RHEL5_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[17:26:03] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.201496.0/RHEL5_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_accuracy <score>=0.58\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_cross_entropy <loss>=0.69142565918\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:03 INFO 140290400139072] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_f_1.000 <score>=0.716599190283\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:04.324] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 578, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=0, train binary_classification_accuracy <score>=0.551813186813\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=0, train binary_classification_cross_entropy <loss>=0.691300354675\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=0, train binary_f_1.000 <score>=0.694601900455\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"update.time\": {\"count\": 1, \"max\": 603.0828952789307, \"sum\": 603.0828952789307, \"min\": 603.0828952789307}}, \"EndTime\": 1559237164.325111, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237163.721712}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}, \"Total Records Seen\": {\"count\": 1, \"max\": 91569, \"sum\": 91569.0, \"min\": 91569}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1559237164.325328, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1559237163.722001}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=150094.282187 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_cross_entropy <loss>=0.675090209961\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_f_1.000 <score>=0.738965952081\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:04.876] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 549, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=1, train binary_classification_accuracy <score>=0.556186813187\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=1, train binary_classification_cross_entropy <loss>=0.687346125005\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=1, train binary_f_1.000 <score>=0.698336582487\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 551.4519214630127, \"sum\": 551.4519214630127, \"min\": 551.4519214630127}}, \"EndTime\": 1559237164.877396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237164.325192}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 183, \"sum\": 183.0, \"min\": 183}, \"Total Records Seen\": {\"count\": 1, \"max\": 182138, \"sum\": 182138.0, \"min\": 182138}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1559237164.877617, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1559237164.325914}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=164126.648876 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_cross_entropy <loss>=0.671481384277\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:04 INFO 140290400139072] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_f_1.000 <score>=0.738965952081\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:05.499] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 619, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #quality_metric: host=algo-1, epoch=2, train binary_classification_accuracy <score>=0.56\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #quality_metric: host=algo-1, epoch=2, train binary_classification_cross_entropy <loss>=0.684615396018\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #quality_metric: host=algo-1, epoch=2, train binary_f_1.000 <score>=0.699385858222\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 622.002124786377, \"sum\": 622.002124786377, \"min\": 622.002124786377}}, \"EndTime\": 1559237165.500199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237164.87746}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 274, \"sum\": 274.0, \"min\": 274}, \"Total Records Seen\": {\"count\": 1, \"max\": 272707, \"sum\": 272707.0, \"min\": 272707}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1559237165.500397, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1559237164.87817}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=145531.176986 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_cross_entropy <loss>=0.668354064941\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:05 INFO 140290400139072] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_f_1.000 <score>=0.738965952081\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:06.104] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 602, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=3, train binary_classification_accuracy <score>=0.564483516484\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=3, train binary_classification_cross_entropy <loss>=0.682148096776\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=3, train binary_f_1.000 <score>=0.700139216754\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 604.3901443481445, \"sum\": 604.3901443481445, \"min\": 604.3901443481445}}, \"EndTime\": 1559237166.105417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237165.500269}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 365, \"sum\": 365.0, \"min\": 365}, \"Total Records Seen\": {\"count\": 1, \"max\": 363276, \"sum\": 363276.0, \"min\": 363276}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1559237166.10568, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1559237165.500993}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=149740.181165 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_cross_entropy <loss>=0.665607177734\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_f_1.000 <score>=0.738965952081\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:06.792] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 684, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=4, train binary_classification_accuracy <score>=0.568175824176\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=4, train binary_classification_cross_entropy <loss>=0.67991609544\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=4, train binary_f_1.000 <score>=0.700222757926\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 687.175989151001, \"sum\": 687.175989151001, \"min\": 687.175989151001}}, \"EndTime\": 1559237166.793642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237166.1055}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 456, \"sum\": 456.0, \"min\": 456}, \"Total Records Seen\": {\"count\": 1, \"max\": 453845, \"sum\": 453845.0, \"min\": 453845}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1559237166.7939, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1559237166.106434}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=131720.648092 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_cross_entropy <loss>=0.66317956543\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:06 INFO 140290400139072] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_f_1.000 <score>=0.738965952081\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:07.429] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 626, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=5, train binary_classification_accuracy <score>=0.572527472527\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=5, train binary_classification_cross_entropy <loss>=0.677882223611\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=5, train binary_f_1.000 <score>=0.701104912944\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 635.7259750366211, \"sum\": 635.7259750366211, \"min\": 635.7259750366211}}, \"EndTime\": 1559237167.43034, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237166.793734}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 547, \"sum\": 547.0, \"min\": 547}, \"Total Records Seen\": {\"count\": 1, \"max\": 544414, \"sum\": 544414.0, \"min\": 544414}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1559237167.430487, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1559237166.794589}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=142404.533482 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_cross_entropy <loss>=0.661020019531\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_f_1.000 <score>=0.738965952081\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:07.990] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 558, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=6, train binary_classification_accuracy <score>=0.575714285714\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=6, train binary_classification_cross_entropy <loss>=0.676013069572\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #quality_metric: host=algo-1, epoch=6, train binary_f_1.000 <score>=0.701410585579\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 559.920072555542, \"sum\": 559.920072555542, \"min\": 559.920072555542}}, \"EndTime\": 1559237167.99094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237167.430392}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 638, \"sum\": 638.0, \"min\": 638}, \"Total Records Seen\": {\"count\": 1, \"max\": 634983, \"sum\": 634983.0, \"min\": 634983}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1559237167.991142, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1559237167.430993}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:07 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=161658.794806 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_accuracy <score>=0.586\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_cross_entropy <loss>=0.659084838867\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_f_1.000 <score>=0.738965952081\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[2019-05-30 17:26:08.529] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 536, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=7, train binary_classification_accuracy <score>=0.579494505495\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=7, train binary_classification_cross_entropy <loss>=0.674279990395\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=7, train binary_f_1.000 <score>=0.702404653767\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 538.672924041748, \"sum\": 538.672924041748, \"min\": 538.672924041748}}, \"EndTime\": 1559237168.530466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237167.991013}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 729, \"sum\": 729.0, \"min\": 729}, \"Total Records Seen\": {\"count\": 1, \"max\": 725552, \"sum\": 725552.0, \"min\": 725552}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1559237168.530666, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1559237167.991764}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=168023.725294 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_accuracy <score>=0.588\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_cross_entropy <loss>=0.657336914063\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:08 INFO 140290400139072] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_f_1.000 <score>=0.739898989899\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:09.016] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 484, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=8, train binary_classification_accuracy <score>=0.582351648352\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=8, train binary_classification_cross_entropy <loss>=0.672658955668\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=8, train binary_f_1.000 <score>=0.703008517621\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 486.1459732055664, \"sum\": 486.1459732055664, \"min\": 486.1459732055664}}, \"EndTime\": 1559237169.017517, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237168.530538}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 820, \"sum\": 820.0, \"min\": 820}, \"Total Records Seen\": {\"count\": 1, \"max\": 816121, \"sum\": 816121.0, \"min\": 816121}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1559237169.017751, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1559237168.531342}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=186148.009822 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_accuracy <score>=0.59\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_cross_entropy <loss>=0.655745056152\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_f_1.000 <score>=0.740506329114\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:09.531] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 511, \"num_examples\": 91, \"num_bytes\": 5796400}\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=9, train binary_classification_accuracy <score>=0.585681318681\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=9, train binary_classification_cross_entropy <loss>=0.671130017626\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, epoch=9, train binary_f_1.000 <score>=0.704083634851\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=0.585681318681\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, train binary_classification_cross_entropy <loss>=0.671130017626\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, train binary_f_1.000 <score>=0.704083634851\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 514.430046081543, \"sum\": 514.430046081543, \"min\": 514.430046081543}}, \"EndTime\": 1559237169.532811, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237169.0176}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Total Batches Seen\": {\"count\": 1, \"max\": 911, \"sum\": 911.0, \"min\": 911}, \"Total Records Seen\": {\"count\": 1, \"max\": 906690, \"sum\": 906690.0, \"min\": 906690}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90569, \"sum\": 90569.0, \"min\": 90569}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1559237169.533039, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1559237169.018354}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #throughput_metric: host=algo-1, train throughput=175926.034198 records/second\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 WARNING 140290400139072] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1.9469261169433594, \"sum\": 1.9469261169433594, \"min\": 1.9469261169433594}}, \"EndTime\": 1559237169.53534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237169.532888}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] Saved checkpoint to \"/tmp/tmpJlVlcs/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:09.542] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 5886, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[31m[2019-05-30 17:26:09.578] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 36, \"num_examples\": 10, \"num_bytes\": 603440}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 9429, \"sum\": 9429.0, \"min\": 9429}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"Total Records Seen\": {\"count\": 1, \"max\": 9429, \"sum\": 9429.0, \"min\": 9429}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 9429, \"sum\": 9429.0, \"min\": 9429}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1559237169.578857, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237169.542098}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #test_score (algo-1) : ('binary_classification_accuracy', 0.58404920988439923)\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #test_score (algo-1) : ('binary_classification_cross_entropy', 0.67472574118908424)\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #test_score (algo-1) : ('binary_f_1.000', 0.7348925239962146)\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, test binary_classification_accuracy <score>=0.584049209884\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, test binary_classification_cross_entropy <loss>=0.674725741189\u001b[0m\n",
      "\u001b[31m[05/30/2019 17:26:09 INFO 140290400139072] #quality_metric: host=algo-1, test binary_f_1.000 <score>=0.734892523996\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 5980.861186981201, \"sum\": 5980.861186981201, \"min\": 5980.861186981201}, \"setuptime\": {\"count\": 1, \"max\": 47.37401008605957, \"sum\": 47.37401008605957, \"min\": 47.37401008605957}}, \"EndTime\": 1559237169.579874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1559237169.535409}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-05-30 17:26:19 Completed - Training job completed\n",
      "Billable seconds: 60\n"
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                   get_execution_role(), \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=nbFeatures,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=10)\n",
    "\n",
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If the training was successful, you wil see 'Training job completed' at the end of the output. Scroll up to see the  train and test accuracy***    \n",
    "\n",
    "***After training phase completed, we have the model parameters stored in S3 (in the output path you specified). You can check your S3 bucket that contains the output to inspect how the training job output looks like***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM7: Deploy model\n",
    "\n",
    "***Now, let's deploy the model for inference using SageMaker SDK. It will spin-up a new virtual machine with container containing algorithm for inference. It will give us an API endpoint for inference.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.t2.medium', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM8: Run predictions\n",
    "\n",
    "***After the model is deployed and given an endpoint, we can run the prediction / inference.***  \n",
    "Below we define the serializer and deserializer for the prediction request/response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        js['instances'].append({'features': row.tolist()})\n",
    "    #print js\n",
    "    return json.dumps(js)\n",
    "\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's test the prediction with some data from the test set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_from = 900\n",
    "index_to = 910\n",
    "result = fm_predictor.predict(X_test[index_from:index_to].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Display the prediction in pretty table, being compared againts the actual rating (label) from the test set.***.      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.8.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Score           </td><td style=\"text-align: right;\">0.61</td><td style=\"text-align: right;\">0.53</td><td style=\"text-align: right;\">0.56</td><td style=\"text-align: right;\">0.55</td><td style=\"text-align: right;\">0.59</td><td style=\"text-align: right;\">0.64</td><td style=\"text-align: right;\">0.59</td><td style=\"text-align: right;\">0.53</td><td style=\"text-align: right;\">0.54</td><td style=\"text-align: right;\">0.53</td></tr>\n",
       "<tr><td>Predicted Rating</td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td></tr>\n",
       "<tr><td>Actual Rating   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">0   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">0   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">0   </td><td style=\"text-align: right;\">0   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tabulate\n",
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "scores, predicted_rating = ['Score'], ['Predicted Rating']\n",
    "for r in result['predictions']:\n",
    "    scores.append(\"%.2f\" % r['score'])\n",
    "    predicted_rating.append(r['predicted_label'])\n",
    "\n",
    "\n",
    "table = [scores, predicted_rating, ['Actual Rating'] + Y_test[index_from:index_to].tolist() ]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step SM9: Get Movies Recommendation\n",
    "\n",
    "***After testing the prediction, let's get real movies recommendation for a particular user***    \n",
    "First, let's prepare a dictionary that maps movie ID to its title. We use the u.item data containing movies' details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = 344\n",
    "score_threshold = 0.50\n",
    "maximum_recommendations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = {}\n",
    "with open('./ml-100k/u.item','r', encoding = \"ISO-8859-1\") as f:\n",
    "    samples=csv.reader(f,delimiter='|')\n",
    "    for movieId,m_title,r_date,video_r_date,imdb_URL,unkwn,act,adv,anm,kid,cmd,crime,doc,drama,fantasy,f_noir,horror,msc,myst,rom,sfy,thriller,war,west in samples:\n",
    "        movies[int(movieId)] = m_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Run predictions for all movies for this particular user and sort the output based on score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_movies=[]\n",
    "for movieId in range(nbMovies):\n",
    "    test_input = lil_matrix((1, nbFeatures)).astype('float32')\n",
    "    test_input[0, int(userId)-1] = 1\n",
    "    test_input[0, nbUsers+int(movieId)-1] = 1\n",
    "    result = fm_predictor.predict(test_input.toarray())\n",
    "    result_label, result_score = int(result['predictions'][0]['predicted_label']), float(result['predictions'][0]['score'])\n",
    "    if (result_label == 1) and (result_score > score_threshold):\n",
    "        recommended_movies.append([int(movieId),result_score])\n",
    "        \n",
    "def getVal(item):\n",
    "    return item[1]\n",
    "recommended_movies = sorted(recommended_movies,key=getVal,reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Print out the result of top recommended movies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td><strong>Movie Title</strong>          </td><td><strong>Score</strong></td></tr>\n",
       "<tr><td>Princess Bride, The (1987)            </td><td>0.6682114005088806    </td></tr>\n",
       "<tr><td>Fargo (1996)                          </td><td>0.6564514636993408    </td></tr>\n",
       "<tr><td>Return of the Jedi (1983)             </td><td>0.6532304286956787    </td></tr>\n",
       "<tr><td>Raiders of the Lost Ark (1981)        </td><td>0.6506448984146118    </td></tr>\n",
       "<tr><td>Aliens (1986)                         </td><td>0.6488392949104309    </td></tr>\n",
       "<tr><td>Snow White and the Seven Dwarfs (1937)</td><td>0.6474416851997375    </td></tr>\n",
       "<tr><td>Shining, The (1980)                   </td><td>0.6474149823188782    </td></tr>\n",
       "<tr><td>Maltese Falcon, The (1941)            </td><td>0.6462321877479553    </td></tr>\n",
       "<tr><td>Clockwork Orange, A (1971)            </td><td>0.6449532508850098    </td></tr>\n",
       "<tr><td>Raging Bull (1980)                    </td><td>0.6435233950614929    </td></tr>\n",
       "<tr><td>Alien (1979)                          </td><td>0.6427415609359741    </td></tr>\n",
       "<tr><td>Empire Strikes Back, The (1980)       </td><td>0.6411765813827515    </td></tr>\n",
       "<tr><td>North by Northwest (1959)             </td><td>0.6410483717918396    </td></tr>\n",
       "<tr><td>2001: A Space Odyssey (1968)          </td><td>0.6402925252914429    </td></tr>\n",
       "<tr><td>Brazil (1985)                         </td><td>0.6360920071601868    </td></tr>\n",
       "<tr><td>Amadeus (1984)                        </td><td>0.634550154209137     </td></tr>\n",
       "<tr><td>Blues Brothers, The (1980)            </td><td>0.6335384845733643    </td></tr>\n",
       "<tr><td>Dead Poets Society (1989)             </td><td>0.6332672238349915    </td></tr>\n",
       "<tr><td>Quiz Show (1994)                      </td><td>0.6318451762199402    </td></tr>\n",
       "<tr><td>Sting, The (1973)                     </td><td>0.6316269040107727    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_table = [['<strong>Movie Title</strong>','<strong>Score</strong>']]\n",
    "for i in range(min(maximum_recommendations,len(recommended_movies))):\n",
    "    output_table.append([movies[int(recommended_movies[i][0])],recommended_movies[i][1]])\n",
    "\n",
    "display(HTML(tabulate.tabulate(output_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesByUser = {}\n",
    "for userId in range(nbUsers):\n",
    "    moviesByUser[str(userId)]=[]\n",
    " \n",
    "with open('train.csv','r') as f:\n",
    "    samples=csv.reader(f,delimiter=',')\n",
    "    for userId,movieId,rating,timestamp, rating_b in samples:\n",
    "        moviesByUser[str(int(userId)-1)].append([int(movieId)-1,rating]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compare the recommendation with the top 20 movies that are actually rated by that particular user, sorted from the highest rating***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td><strong>Movie Title</strong>    </td><td><strong>Actual Rating</strong></td></tr>\n",
       "<tr><td>GoldenEye (1995)                </td><td>5                             </td></tr>\n",
       "<tr><td>Usual Suspects, The (1995)      </td><td>5                             </td></tr>\n",
       "<tr><td>Clerks (1994)                   </td><td>5                             </td></tr>\n",
       "<tr><td>Professional, The (1994)        </td><td>5                             </td></tr>\n",
       "<tr><td>Pulp Fiction (1994)             </td><td>5                             </td></tr>\n",
       "<tr><td>Shawshank Redemption, The (1994)</td><td>5                             </td></tr>\n",
       "<tr><td>Forrest Gump (1994)             </td><td>5                             </td></tr>\n",
       "<tr><td>Fugitive, The (1993)            </td><td>5                             </td></tr>\n",
       "<tr><td>True Romance (1993)             </td><td>5                             </td></tr>\n",
       "<tr><td>Silence of the Lambs, The (1991)</td><td>5                             </td></tr>\n",
       "<tr><td>Fargo (1996)                    </td><td>5                             </td></tr>\n",
       "<tr><td>Godfather, The (1972)           </td><td>5                             </td></tr>\n",
       "<tr><td>Princess Bride, The (1987)      </td><td>5                             </td></tr>\n",
       "<tr><td>GoodFellas (1990)               </td><td>5                             </td></tr>\n",
       "<tr><td>Army of Darkness (1993)         </td><td>5                             </td></tr>\n",
       "<tr><td>Godfather: Part II, The (1974)  </td><td>5                             </td></tr>\n",
       "<tr><td>Sting, The (1973)               </td><td>5                             </td></tr>\n",
       "<tr><td>Dead Poets Society (1989)       </td><td>5                             </td></tr>\n",
       "<tr><td>Evil Dead II (1987)             </td><td>5                             </td></tr>\n",
       "<tr><td>Patton (1970)                   </td><td>5                             </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_top_rated_movies(user_id, k):\n",
    "    rated_movies = moviesByUser[str(int(user_id)-1)]\n",
    "    rated_movies = sorted(rated_movies,key=getVal,reverse=True)\n",
    "    results = []\n",
    "    \n",
    "    for movie in rated_movies:\n",
    "        results.append([movies[int(movie[0]+1)],movie[1]])\n",
    "    return results[0:k]\n",
    "\n",
    "output_table = [['<strong>Movie Title</strong>','<strong>Actual Rating</strong>']]\n",
    "for m in find_top_rated_movies(userId,20):\n",
    "    output_table.append(m)\n",
    "\n",
    "display(HTML(tabulate.tabulate(output_table, tablefmt='html')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
