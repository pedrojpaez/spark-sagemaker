{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook will show how to use the PySpark on a Sagemaker notebook. \n",
    "\n",
    "We will manipulate data through Spark using a SparkSession and upload the resulting dataframe to S3.\n",
    "\n",
    "\n",
    "You can visit SageMaker Spark's GitHub repository at https://github.com/aws/sagemaker-spark to learn more about SageMaker Spark.\n",
    "\n",
    "This notebook was created and tested on an ml.m4.xlarge notebook instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker_pyspark\n",
    "from sagemaker_pyspark import *\n",
    "from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator, PCASageMakerEstimator\n",
    "from sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\n",
    "from sagemaker_pyspark.transformation.deserializers import KMeansProtobufResponseRowDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can now create the SparkSession with the SageMaker-Spark dependencies attached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-64-16.ap-southeast-1.compute.internal:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbaeae39978>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "# See the SageMaker Spark Github to learn how to connect to EMR from a notebook instance\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local[*]\").getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading the Data\n",
    "\n",
    "Now, we load the MovieLens dataset into a Spark Dataframe.\n",
    "\n",
    "Here, we load into a DataFrame in the SparkSession running on the local Notebook Instance, but you can connect your Notebook Instance to a remote Spark cluster for heavier workloads. Starting from EMR 5.11.0, SageMaker Spark is pre-installed on EMR Spark clusters. For more on connecting your SageMaker Notebook Instance to a remote EMR cluster, please see this blog post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://pedro-spark-sagemaker/raw/train.csv\n"
     ]
    }
   ],
   "source": [
    "S3_BUCKET = \"pedro-spark-sagemaker\"\n",
    "S3_TARGET_PREFIX = \"/raw/train.csv\"\n",
    "S3_LOCATION = \"s3a://\"+S3_BUCKET+S3_TARGET_PREFIX \n",
    "print(S3_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf, lit\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from time import mktime, strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-30 16:53:33--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip.1’\n",
      "\n",
      "ml-100k.zip.1       100%[===================>]   4.70M  2.87MB/s    in 1.6s    \n",
      "\n",
      "2019-05-30 16:53:35 (2.87 MB/s) - ‘ml-100k.zip.1’ saved [4924029/4924029]\n",
      "\n",
      "Archive:  ml-100k.zip\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    }
   ],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ml-100k/ua.base to s3://pedro-spark-sagemaker/raw/train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp './ml-100k/ua.base' 's3://pedro-spark-sagemaker/raw/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 182.3 KiB/182.3 KiB (1.6 MiB/s) with 1 file(s) remaining\r",
      "upload: ml-100k/ua.test to s3://pedro-spark-sagemaker/raw/test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp './ml-100k/ua.test' 's3://pedro-spark-sagemaker/raw/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE train/\r\n",
      "2019-05-30 06:11:21   48039726 RetailData.csv\r\n",
      "2019-05-28 01:40:00   23715344 ecommerce-data.csv\r\n",
      "2019-05-30 16:53:44     186672 test.csv\r\n",
      "2019-05-30 16:53:43    1792501 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://pedro-spark-sagemaker/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the movielens data set to a Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|USER_ID|ITEM_ID|RATING|TIMESTAMP|\n",
      "+-------+-------+------+---------+\n",
      "|      1|      2|     3|876893171|\n",
      "|      1|      3|     4|878542960|\n",
      "|      1|      4|     3|876893119|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoiceSchema =  StructType([StructField('USER_ID', StringType(), False),\n",
    "                    StructField('ITEM_ID', StringType(), False),\n",
    "                    StructField('RATING', StringType(), True),\n",
    "                    StructField('TIMESTAMP', StringType())])\n",
    "\n",
    "invoicesDf=spark.read.schema(invoiceSchema).option(\"delimiter\", \"\\t\").options(header=True).csv(S3_LOCATION)\n",
    "invoicesDf.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll do some data preprocessing using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoicesDf.registerTempTable(\"invoices\")\n",
    "invoicesCleanDf = sqlContext.sql(\n",
    "    \"select * from invoices where RATING is null\"\n",
    ")\n",
    "invoicesCleanDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column binarizing the ratings ( >=4 is 1 else 0). We will use both rating systems for creating our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90569"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "invoicesCleanDf = sqlContext.sql(\n",
    "    \"select *, \\\n",
    "    CASE \\\n",
    "        WHEN RATING >3 THEN 1 \\\n",
    "        ELSE 0 \\\n",
    "    END AS RATING_B \\\n",
    "    from invoices \\\n",
    "    where USER_ID is not null and ITEM_ID is not null and RATING > 0\"\n",
    ")\n",
    "invoicesCleanDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+--------+\n",
      "|USER_ID|ITEM_ID|RATING|TIMESTAMP|RATING_B|\n",
      "+-------+-------+------+---------+--------+\n",
      "|      1|      2|     3|876893171|       0|\n",
      "|      1|      3|     4|878542960|       1|\n",
      "|      1|      4|     3|876893119|       0|\n",
      "|      1|      5|     3|889751712|       0|\n",
      "|      1|      6|     5|887431973|       1|\n",
      "+-------+-------+------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoicesCleanDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeriesDF=invoicesCleanDf.withColumn(\"USER_ID\", invoicesCleanDf[\"USER_ID\"].cast(IntegerType())).withColumn(\"ITEM_ID\", invoicesCleanDf[\"ITEM_ID\"].cast(IntegerType())).withColumn(\"RATING\", invoicesCleanDf[\"RATING\"].cast(IntegerType())).withColumn(\"RATING_B\", invoicesCleanDf[\"RATING_B\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the resulting dataframe to S3 for use for our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://pedro-spark-sagemaker/raw/train\n"
     ]
    }
   ],
   "source": [
    "S3_TARGET_PREFIX = \"/raw/train\"\n",
    "S3_LOCATION = \"s3a://\"+S3_BUCKET+S3_TARGET_PREFIX\n",
    "\n",
    "print(S3_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeriesDF.coalesce(1).write.csv(S3_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the test data set as well (movielens data already has train/test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|USER_ID|ITEM_ID|RATING|TIMESTAMP|\n",
      "+-------+-------+------+---------+\n",
      "|      1|     33|     4|878542699|\n",
      "|      1|     61|     4|878542420|\n",
      "|      1|    117|     3|874965739|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf=spark.read.schema(invoiceSchema).option(\"delimiter\", \"\\t\").options(header=True).csv('s3a://pedro-spark-sagemaker/raw/test.csv')\n",
    "testDf=testDf.withColumn(\"USER_ID\", testDf[\"USER_ID\"].cast(IntegerType())).withColumn(\"ITEM_ID\", testDf[\"ITEM_ID\"].cast(IntegerType())).withColumn(\"RATING\", testDf[\"RATING\"].cast(IntegerType()))\n",
    "testDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9429"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDf.registerTempTable(\"test\")\n",
    "testfinalDf = sqlContext.sql(\n",
    "    \"select *, \\\n",
    "    CASE \\\n",
    "        WHEN RATING >3 THEN 1 \\\n",
    "        ELSE 0 \\\n",
    "    END AS RATING_B \\\n",
    "    from test \\\n",
    "    where USER_ID is not null and ITEM_ID is not null and RATING > 0\"\n",
    ")\n",
    "testfinalDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+--------+\n",
      "|USER_ID|ITEM_ID|RATING|TIMESTAMP|RATING_B|\n",
      "+-------+-------+------+---------+--------+\n",
      "|      1|     33|     4|878542699|       1|\n",
      "|      1|     61|     4|878542420|       1|\n",
      "|      1|    117|     3|874965739|       0|\n",
      "|      1|    155|     2|878542201|       0|\n",
      "+-------+-------+------+---------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testfinalDf.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload test dataframe to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfinalDf.coalesce(1).write.csv('s3a://pedro-spark-sagemaker/raw/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ALS collaborative filtering model using MLlib from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"USER_ID\", itemCol=\"ITEM_ID\", ratingCol=\"RATING\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(timeSeriesDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+----------+\n",
      "|USER_ID|ITEM_ID|RATING|TIMESTAMP|prediction|\n",
      "+-------+-------+------+---------+----------+\n",
      "|    251|    148|     2|886272547| 3.1896665|\n",
      "|    580|    148|     4|884125773| 3.6820707|\n",
      "|    602|    148|     4|888638517| 4.4973254|\n",
      "|    372|    148|     5|876869915| 3.3083372|\n",
      "|    274|    148|     2|878946133| 2.9454553|\n",
      "|    923|    148|     4|880387474| 3.0570798|\n",
      "|    447|    148|     4|878854729| 2.9873352|\n",
      "|    586|    148|     3|884065745|  3.355267|\n",
      "|    761|    148|     5|876189829| 3.6957355|\n",
      "|    677|    148|     4|889399265| 2.7402072|\n",
      "+-------+-------+------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"RATING\",\n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.235444431262701\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
